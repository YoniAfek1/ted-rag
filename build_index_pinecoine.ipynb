{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5dfb94",
   "metadata": {},
   "source": [
    "**Pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\97250\\AppData\\Local\\Temp\\ipykernel_30584\\162050665.py:103: DtypeWarning: Columns (24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(FILE_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4016 talks.\n",
      "Processing text into chunks and extracting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 4016/4016 [00:07<00:00, 550.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 23549\n",
      "Creating new Pinecone index: ted-rag-index...\n",
      "Starting Embedding & Upsert process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting Batches:  99%|█████████▉| 234/236 [31:00<00:17,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch starting at index 23300: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sun, 14 Dec 2025 11:31:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '173', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '2189', 'x-pinecone-request-id': '539710761821096387', 'x-envoy-upstream-service-time': '32', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector ID must be ASCII, but got 'a lot of cases in children — is that because the children get infected but they don't get symptoms_0'\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting Batches: 100%|██████████| 236/236 [31:11<00:00,  7.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline completed successfully! Data is ready in Pinecone.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Configuration & Constants\n",
    "# ---------------------------------------------------------\n",
    "# Set your keys here (or load from environment variables)\n",
    "os.environ['OPENAI_API_KEY'] = 'ENTER YOUR API KEY HERE'\n",
    "os.environ['PINECONE_API_KEY'] = 'ENTER YOUR PINECONE API KEY HERE'\n",
    "\n",
    "# Assignment configuration\n",
    "FILE_PATH = 'ted_talks_en.csv'\n",
    "INDEX_NAME = \"ted-rag-index\"\n",
    "EMBEDDING_MODEL = \"RPRTHPB-text-embedding-3-small\"   # LLMod compatible\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "\n",
    "# --- Updated Parameters per our conversation ---\n",
    "MAX_CHUNK_SIZE = 512   # Updated to 512 for better semantic context in narrative text\n",
    "OVERLAP_RATIO = 0.2    # 20% overlap\n",
    "BATCH_SIZE = 100       # Efficiency batch size\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Client Initialization\n",
    "# ---------------------------------------------------------\n",
    "# LLMod.ai API-compatible client\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    base_url=\"https://api.llmod.ai/v1\" \n",
    ")\n",
    "\n",
    "# Pinecone\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Helper Functions (Tokenization & Safety)\n",
    "# ---------------------------------------------------------\n",
    "def get_text_chunks(text: str, chunk_size: int, overlap_ratio: float) -> List[str]:\n",
    "    \"\"\"Splits text into chunks based on token count with overlap.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "        \n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    step = int(chunk_size * (1 - overlap_ratio))\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, total_tokens, step):\n",
    "        chunk_tokens = tokens[i : i + chunk_size]\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def safe_str(val):\n",
    "    \"\"\"Safely converts value to string, handling NaNs.\"\"\"\n",
    "    return str(val) if pd.notna(val) else \"\"\n",
    "\n",
    "def safe_int(val):\n",
    "    \"\"\"Safely converts value to int, defaulting to 0.\"\"\"\n",
    "    try:\n",
    "        return int(val) if pd.notna(val) else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Pinecone Index Setup\n",
    "# ---------------------------------------------------------\n",
    "def setup_pinecone_index(index_name: str, dimension: int):\n",
    "    existing_indexes = [i.name for i in pc.list_indexes()]\n",
    "    \n",
    "    if index_name not in existing_indexes:\n",
    "        print(f\"Creating new Pinecone index: {index_name}...\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        time.sleep(10) # Wait for initialization\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists. Connecting...\")\n",
    "    \n",
    "    return pc.Index(index_name)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Main ETL Pipeline\n",
    "# ---------------------------------------------------------\n",
    "def run_pipeline():\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"Error: File {FILE_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    print(f\"Loaded {len(df)} talks.\")\n",
    "\n",
    "    # Prepare chunks with all metadata\n",
    "    all_chunks_data = []\n",
    "    print(\"Processing text into chunks and extracting metadata...\")\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking\"):\n",
    "        # Basic fields\n",
    "        talk_id = safe_str(row.get('talk_id'))\n",
    "        title = safe_str(row.get('title'))\n",
    "        transcript = safe_str(row.get('transcript'))\n",
    "        \n",
    "        # Skip if empty transcript\n",
    "        if not transcript:\n",
    "            continue\n",
    "            \n",
    "        chunks = get_text_chunks(transcript, MAX_CHUNK_SIZE, OVERLAP_RATIO)\n",
    "        \n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            # Create unique ID\n",
    "            chunk_id = f\"{talk_id}_{i}\"\n",
    "            \n",
    "            # --- Capture all new fields for Metadata ---\n",
    "            metadata = {\n",
    "                \"talk_id\": talk_id,\n",
    "                \"title\": title,\n",
    "                \"chunk_text\": chunk_text, \n",
    "                \"chunk_index\": i,\n",
    "                \n",
    "                # Extended Metadata from CSV\n",
    "                \"url\": safe_str(row.get('url')),\n",
    "                \"speaker\": safe_str(row.get('speaker_1')),\n",
    "                \"topics\": safe_str(row.get('topics')),\n",
    "                \"views\": safe_int(row.get('views')),\n",
    "                \"published_date\": safe_str(row.get('published_date')),\n",
    "                \"duration\": safe_int(row.get('duration')),\n",
    "                \"event\": safe_str(row.get('event')),\n",
    "                \"native_language\": safe_str(row.get('native_language'))\n",
    "            }\n",
    "\n",
    "            all_chunks_data.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "    \n",
    "    total_chunks = len(all_chunks_data)\n",
    "    print(f\"Total chunks created: {total_chunks}\")\n",
    "\n",
    "    # Initialize Pinecone\n",
    "    index = setup_pinecone_index(INDEX_NAME, EMBEDDING_DIMENSIONS)\n",
    "\n",
    "    # Generate Embeddings and Upsert in Batches\n",
    "    print(\"Starting Embedding & Upsert process...\")\n",
    "    for i in tqdm(range(0, total_chunks, BATCH_SIZE), desc=\"Upserting Batches\"):\n",
    "        batch_slice = all_chunks_data[i : i + BATCH_SIZE]\n",
    "        batch_texts = [item['text'] for item in batch_slice]\n",
    "        \n",
    "        try:\n",
    "            # 1. Generate Embeddings\n",
    "            response = client.embeddings.create(\n",
    "                input=batch_texts,\n",
    "                model=EMBEDDING_MODEL\n",
    "            )\n",
    "            embeddings = [data.embedding for data in response.data]\n",
    "            \n",
    "            # 2. Prepare vectors\n",
    "            vectors_to_upsert = []\n",
    "            for j, item in enumerate(batch_slice):\n",
    "                vectors_to_upsert.append({\n",
    "                    \"id\": item['id'],\n",
    "                    \"values\": embeddings[j],\n",
    "                    \"metadata\": item['metadata']\n",
    "                })\n",
    "            \n",
    "            # 3. Upsert to Pinecone\n",
    "            index.upsert(vectors=vectors_to_upsert)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "\n",
    "    print(\"\\nPipeline completed successfully! Data is ready in Pinecone.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Execution\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
